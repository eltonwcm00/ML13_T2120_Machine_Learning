{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bc6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "batch_size = 128 \n",
    "img_width, img_height, img_num_channels = 32, 32, 3\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 100\n",
    "no_epochs = 100\n",
    "optimizer = Adam()\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "# MLP\n",
    "weight_decay = 0.0001\n",
    "dropout_rate = 0.2\n",
    "image_size = 64  # We'll resize input images to this size. #72 \n",
    "patch_size = 8  # Size of the patches to be extracted from the input images. #6\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
    "embedding_dim = 256  # Number of hidden units.\n",
    "num_blocks = 4  # Number of blocks.\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 data\n",
    "# (input_train, target_train), (input_test, target_test) = cifar100.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "# Parse numbers as floats\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Determine shape of the data\n",
    "input_shape = (img_width, img_height, img_num_channels)\n",
    "# Normalize data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7457fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['beetle', 'spider', 'road', 'orchid', 'motorcycle', 'orange', 'cattle', 'fox', 'wolf', 'turtle', 'snake', 'shark', 'cloud', 'hamster', 'mountain', 'bus', 'boy', 'skunk', 'clock', 'chimpanzee', 'tank', 'can', 'dinosaur', 'lawn_mower', 'butterfly', 'man', 'lion', 'lobster', 'otter', 'television', 'apple', 'raccoon', 'plain', 'telephone', 'oak_tree', 'skyscraper', 'flatfish', 'bear', 'cup', 'bee', 'trout', 'wardrobe', 'sweet_pepper', 'caterpillar', 'rocket', 'tractor', 'whale', 'pear', 'mouse', 'tiger', 'lizard', 'plate', 'lamp', 'porcupine', 'kangaroo', 'poppy', 'willow_tree', 'bed', 'house', 'worm', 'keyboard', 'aquarium_fish', 'possum', 'beaver', 'pickup_truck', 'elephant', 'baby', 'snail', 'castle', 'seal', 'mushroom', 'bridge', 'woman', 'bicycle', 'streetcar', 'rabbit', 'crab', 'girl', 'chair', 'sea', 'palm_tree', 'dolphin', 'camel', 'maple_tree', 'squirrel', 'leopard', 'cockroach', 'forest', 'crocodile', 'bowl', 'sunflower', 'tulip', 'bottle', 'couch', 'rose', 'ray', 'train', 'shrew', 'pine_tree', 'table']\n",
    "print(\"Number of Classes :\", len(class_names))\n",
    "print(\"Class names: {}\".format(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========\n",
    "# CNN Model\n",
    "# ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0970099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c914453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate generalization metrics\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize history\n",
    "# Plot history: Training loss \n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7809b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize history\n",
    "# Plot history: Validation loss \n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Validation loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Training Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Training accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Validation Accuracy\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Validation accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1232722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========\n",
    "# MLP Model\n",
    "# ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01e12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(blocks, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks.\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation)\n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0f12727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay.\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=no_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bda22576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e2c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement patch creation as layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e8245d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.mlp2 = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebeb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "352/352 [==============================] - 394s 1s/step - loss: 3.8699 - acc: 0.1143 - top5-acc: 0.3190 - val_loss: 3.5618 - val_acc: 0.1804 - val_top5-acc: 0.4394 - lr: 0.0050\n",
      "Epoch 2/2\n",
      "111/352 [========>.....................] - ETA: 4:21 - loss: 3.4751 - acc: 0.1686 - top5-acc: 0.4370"
     ]
    }
   ],
   "source": [
    "mlpmixer_blocks = keras.Sequential(\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
    ")\n",
    "learning_rate = 0.005\n",
    "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
    "history = run_experiment(mlpmixer_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate generalization metrics\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize history\n",
    "# Plot history: Training Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize history\n",
    "# Plot history: Validation Loss\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Validation loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Training Accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Training accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Validation Accuracy\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Validation accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 89,num=90)\n",
    "plt.imshow(confmat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e19acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
